{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pymatgen'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11500\\1616175855.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpymatgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpymat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmendeleev\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmendel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pymatgen'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# You may not be able to install tensorflow using conda install inside pytmatgen.\n",
    "# use pip install tensorflow\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import pymatgen.core as pymat\n",
    "import mendeleev as mendel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, '../src/')\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Getting a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcc_elements = [\"Ag\", \"Al\", \"Au\", \"Cu\", \"Ir\", \"Ni\", \"Pb\", \"Pd\", \"Pt\", \"Rh\", \"Th\", \"Yb\"]\n",
    "bcc_elements = [\"Ba\", \"Cr\", \"Eu\", \"Fe\", \"Li\", \"Mn\", \"Mo\", \"Na\", \"Nb\", \"Ta\", \"V\", \"W\" ]\n",
    "hcp_elements = [\"Be\", \"Ca\", \"Cd\", \"Co\", \"Dy\", \"Er\", \"Gd\", \"Hf\", \"Ho\", \"Lu\", \"Mg\", \"Re\", \n",
    "                \"Sc\", \"Tb\", \"Ti\", \"Tl\", \"Tm\", \"Y\", \"Zn\"]\n",
    "\n",
    "elements = fcc_elements + bcc_elements + hcp_elements\n",
    "\n",
    "querable_mendeleev = [\"atomic_number\", \"atomic_volume\", \"boiling_point\",\n",
    "                      \"en_ghosh\",  \"evaporation_heat\", \"heat_of_formation\",\n",
    "                     \"lattice_constant\", \"specific_heat\"]\n",
    "\n",
    "# en_ghosh is Ghosh's scale of electronegativity. See this: https://doi.org/10.1142/S0219633605001556\n",
    "# There are many such scale.\n",
    "\n",
    "querable_pymatgen = [\"atomic_mass\", \"atomic_radius\", \"electrical_resistivity\",\n",
    "                     \"molar_volume\", \"bulk_modulus\", \"youngs_modulus\",\n",
    "                     \"average_ionic_radius\", \"density_of_solid\",\n",
    "                     \"coefficient_of_linear_thermal_expansion\"]\n",
    "\n",
    "querable_values = querable_mendeleev + querable_pymatgen\n",
    "\n",
    "# Note that in this examples, two different APIs are used and two different sets of features have been collected.\n",
    "# These features have been collated in a single set of queable values.\n",
    "#np.shape(elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Querying the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_values = [] # Values for Attributes\n",
    "all_labels = [] # Values for Young's Modulus (Property to be estimated)\n",
    "\n",
    "# This neural network will produce a virtual function that will give Young's modulus E\n",
    "#E = E(values)\n",
    "#E = E(atomic number, atomic mass, boiling point......)\n",
    "\n",
    "for item in elements:\n",
    "    element_values = []\n",
    "    \n",
    "    # This section queries Mendeleev\n",
    "    element_object = mendel.element(item)\n",
    "    for i in querable_mendeleev:    \n",
    "        element_values.append(getattr(element_object,i))\n",
    "\n",
    "    # This section queries Pymatgen\n",
    "    element_object = pymat.Element(item)    \n",
    "    for i in querable_pymatgen:\n",
    "        element_values.append(getattr(element_object,i))\n",
    "        \n",
    "    all_values.append(element_values) # All lists are appended to another list, creating a list of lists\n",
    "    \n",
    "# Pandas Dataframe\n",
    "df = pd.DataFrame(all_values, columns=querable_values)\n",
    "\n",
    "\n",
    "# The labels (values for Young's modulus) are stored separately for clarity (We drop the column later)\n",
    "\n",
    "df.to_csv(os.path.expanduser('element_data.csv'), index=False, compression=None) # this line saves the data we collected into a .csv file into your home directory\n",
    "\n",
    "all_labels = df['youngs_modulus'].tolist()\n",
    "\n",
    "df = df.drop(['youngs_modulus'], axis=1)\n",
    "\n",
    "#df.head(n=10) # With this line you can see the first ten entries of our database\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Processing and Organizing Data\n",
    "\n",
    "Most machine learning models are trained on a subset of all the available data, called the \"training set\", and the models are tested on the remainder of the available data, called the \"testing set\". Model performance has often been found to be enhanced when the inputs are normalized.\n",
    "\n",
    "##### SETS\n",
    "\n",
    "With the dataset we just created, we have 44 entries for our model. We will train with 39 cases and test on the remaining 5 elements to estimate Young's Modulus.\n",
    "\n",
    "##### NORMALIZATION\n",
    "\n",
    "Each one of these input data features has different units and is represented in scales with distinct orders of magnitude. Datasets that contain inputs like this need to be normalized, so that quantities with large values do not bias the neural network, forcing it tune its weights to account for the different scales of our input data. In this tutorial, we will use the Standard Score Normalization, which subtracts the mean of the feature and divide by its standard deviation.\n",
    "\n",
    "<span style=\"font-size:2em;\">$ \\frac{X - µ}{σ} $ </span>\n",
    "\n",
    "While our model might converge without feature normalization, the resultant model would be difficult to train and would be dependent on the choice of units used in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will rewrite the arrays with the patches we made on the dataset by turning the dataframe back into a list of lists\n",
    "\n",
    "all_values = [list(df.iloc[x]) for x in range(len(all_values))]\n",
    "# all_values is a list of lists.\n",
    "\n",
    "# read about df.iloc here: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html\n",
    "\n",
    "# SETS\n",
    "\n",
    "# List of lists are turned into Numpy arrays to facilitate calculations in steps to follow (Normalization).\n",
    "all_values = np.array(all_values, dtype = float) \n",
    "print(\"Shape of Values:\", all_values.shape)\n",
    "all_labels = np.array(all_labels, dtype = float)\n",
    "print(\"Shape of Labels:\", all_labels.shape)\n",
    "\n",
    "# Uncomment the line below to shuffle the dataset (we do not do this here to ensure consistent results for every run)\n",
    "#order = np.argsort(np.random.random(all_labels.shape)) # This numpy argsort returns the indexes that would be used to shuffle a list\n",
    "order = np.arange(43)\n",
    "all_values = all_values[order]\n",
    "all_labels = all_labels[order]\n",
    "\n",
    "# Training Set, this is a manual partition. If we do the above shuffling, there will be different training set\n",
    "# for each training set.\n",
    "# We have 43 elements in our dataframe i.e. numpy dataset now. Our of those, 38 are used for training.\n",
    "# Training data includes actual training or 'fitting' and validation. This validation is done on-the-fly using\n",
    "# some of the data from training data set. It is called validation dataset. In this examples we have 10 % data\n",
    "# used for validation.\n",
    "# So, training data. = actual training data + validation data\n",
    "\n",
    "train_labels = all_labels[:38]\n",
    "train_values = all_values[:38]\n",
    "\n",
    "# Testing Set. This dataset is not seen at all by the model before it is fitted.\n",
    "test_labels = all_labels[-5:]\n",
    "test_values = all_values[-5:]\n",
    "\n",
    "# This line is used for labels in the plots at the end of the tutorial - Testing Set\n",
    "labeled_elements = [elements[x] for x in order[-5:]] \n",
    "elements = [elements[x] for x in order]\n",
    "\n",
    "# NORMALIZATION\n",
    "\n",
    "mean = np.mean(train_values, axis = 0) # mean, calculating mean for each row.\n",
    "std = np.std(train_values, axis = 0) # standard deviation\n",
    "\n",
    "train_values = (train_values - mean) / std # input scaling\n",
    "test_values = (test_values - mean) / std # input scaling\n",
    "\n",
    "mean_all = np.mean(all_values, axis = 0) # mean, calculating mean for each row.\n",
    "std_all = np.std(all_values, axis = 0) # standard deviation\n",
    "all_values_normalized = (all_values - mean_all) / std_all # input scaling\n",
    "\n",
    "print(train_values[0]) # print a sample entry from the training set\n",
    "print(test_values[0]) # print a sample entry from the testing set\n",
    "print(order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Defining the Model, Training, Testing and Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For learning rate 0.001, epochs 20000 and other default setup, run the model for RMSProp, Adam and Gradient Descent and compare the loss vs. epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSProp Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# DEFINITION OF THE MODEL\n",
    "# The weights of our neural network will be initialized in a random manner, using a seed allows for reproducibility\n",
    "kernel_init = initializers.RandomNormal(seed=0) # initialization of seed will always produce same result.\n",
    "bias_init = initializers.Zeros()\n",
    "\n",
    "model = Sequential() # one input layer and one output layer\n",
    "# https://keras.io/guides/sequential_model/\n",
    "\n",
    "model.add(Dense(16, activation='relu', input_shape=(train_values.shape[1], ), kernel_initializer=kernel_init, \n",
    "                bias_initializer=bias_init))\n",
    "model.add(Dense(32, activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init))\n",
    "model.add(Dense(128, activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init))\n",
    "# model.add(Dense(128, activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init))\n",
    "model.add(Dense(1, kernel_initializer=kernel_init, bias_initializer=bias_init))\n",
    "\n",
    "EPOCHS = 20000 # Number of EPOCHS\n",
    "my_learning_rate = 0.001\n",
    "my_decay_rate = my_learning_rate / EPOCHS\n",
    "\n",
    "# CHOOSING THE OPTIMIZER\n",
    "\n",
    "optimizer = optimizers.RMSprop(learning_rate=my_learning_rate,decay=my_decay_rate) # RMSProp. Initial learning rate has been set to 0.001\n",
    "# optimizer = optimizers.Adam(0.001) # AdaM\n",
    "# optimizer = optimizers.SGD(0.001) # SGD\n",
    "#\n",
    "# This line matches the optimizer to the model and states which metrics will evaluate the model's accuracy\n",
    "model.compile(loss='mae', optimizer=optimizer, metrics=['mae'])\n",
    "# mae is Mean Absolute Error\n",
    "# Read more about regression losses: https://keras.io/api/losses/regression_losses/\n",
    "\n",
    "model.summary() # this will show a chart of the 'architecture' of the model i.e. arrangement of layers etc.\n",
    "\n",
    "#TRAINING\n",
    "# EPOCH REAL TIME COUNTER CLASS\n",
    "class PrintEpNum(keras.callbacks.Callback): # This is a function for the Epoch Counter\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write(\"Current Epoch: \" + str(epoch+1) + \" Training Loss: \" + \"%4f\" %logs.get('loss') \n",
    "                         + \" Validation Loss: \" + \"%4f\" %logs.get('val_loss')\n",
    "                         + '                                       \\r') # Updates current Epoch Number\n",
    "# HISTORY Object which contains how the model learned\n",
    "\n",
    "# Training Values (Properties), Training Labels (Known Young's Moduli) \n",
    "history = model.fit(train_values, train_labels, batch_size=train_values.shape[0], \n",
    "                    epochs=EPOCHS, verbose = False, shuffle=False, validation_split=0.1, callbacks=[PrintEpNum()])\n",
    "\n",
    "# Validation split is the 10 % of training data which will be used for on-the-fly 'verification' of fit.\n",
    "\n",
    "# PLOTTING HISTORY USING MATPLOTLIB\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Abs Error')\n",
    "plt.plot(history.epoch, np.array(history.history['loss']),label='Loss on training set') \n",
    "plt.plot(history.epoch, np.array(history.history['val_loss']),label = 'Validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the predictions of training data by the model\n",
    "train_labels_predict=model.predict(train_values)\n",
    "#\n",
    "plt.scatter(train_labels,train_labels_predict)\n",
    "plt.ylabel('Predicted Taining Data',fontsize=20)\n",
    "plt.xlabel('True Training Data',fontsize=20)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.xlim(0,120)\n",
    "plt.ylim(0,120)\n",
    "plt.axline([0, 0], [1, 1],color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check the predictions of only test data by the model\n",
    "test_labels_predict=model.predict(test_values)\n",
    "#np.shape(test_labels_predict)\n",
    "#np.shape(test_labels)\n",
    "# Plot the predictions of only test data by the model\n",
    "plt.scatter(test_labels,test_labels_predict)\n",
    "plt.ylabel('Predicted Test Data',fontsize=20)\n",
    "plt.xlabel('True Test Data',fontsize=20)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.xlim(0,120)\n",
    "plt.ylim(0,120)\n",
    "plt.axline([0, 0], [1, 1],color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adam Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# DEFINITION OF THE MODEL\n",
    "# The weights of our neural network will be initialized in a random manner, using a seed allows for reproducibility\n",
    "kernel_init = initializers.RandomNormal(seed=0)\n",
    "bias_init = initializers.Zeros()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(16, activation='relu', input_shape=(train_values.shape[1], ), kernel_initializer=kernel_init, \n",
    "                bias_initializer=bias_init))\n",
    "model.add(Dense(32, activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init))\n",
    "model.add(Dense(128, activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init))\n",
    "# model.add(Dense(128, activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init))\n",
    "model.add(Dense(1, kernel_initializer=kernel_init, bias_initializer=bias_init))\n",
    "\n",
    "# DEFINITION OF THE OPTIMIZER\n",
    "EPOCHS = 20000 # Number of EPOCHS\n",
    "my_learning_rate = 0.001\n",
    "my_decay_rate = my_learning_rate / EPOCHS\n",
    "\n",
    "\n",
    "# optimizer = optimizers.RMSprop(0.001) # RMSProp\n",
    "optimizer = optimizers.Adam(learning_rate=my_learning_rate,decay=my_decay_rate)#0.001) # AdaM\n",
    "# optimizer = optimizers.SGD(0.001) # SGD\n",
    "\n",
    "# This line matches the optimizer to the model and states which metrics will evaluate the model's accuracy\n",
    "model.compile(loss='mae', optimizer=optimizer, metrics=['mae'])\n",
    "model.summary()\n",
    "\n",
    "#TRAINING\n",
    "# EPOCH REAL TIME COUNTER CLASS\n",
    "class PrintEpNum(keras.callbacks.Callback): # This is a function for the Epoch Counter\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write(\"Current Epoch: \" + str(epoch+1) + \" Training Loss: \" + \"%4f\" %logs.get('loss') \n",
    "                         + \" Validation Loss: \" + \"%4f\" %logs.get('val_loss')\n",
    "                         + '                                       \\r') # Updates current Epoch Number\n",
    "\n",
    "EPOCHS = 20000 # Number of EPOCHS\n",
    "\n",
    "# HISTORY Object which contains how the model learned\n",
    "\n",
    "# Training Values (Properties), Training Labels (Known Young's Moduli) \n",
    "history = model.fit(train_values, train_labels, batch_size=train_values.shape[0], \n",
    "                    epochs=EPOCHS, verbose = False, shuffle=False, validation_split=0.1, callbacks=[PrintEpNum()])\n",
    "\n",
    "\n",
    "# PLOTTING HISTORY USING MATPLOTLIB\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Abs Error')\n",
    "plt.plot(history.epoch, np.array(history.history['loss']),label='Loss on training set') \n",
    "plt.plot(history.epoch, np.array(history.history['val_loss']),label = 'Validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions by ADAM optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the predictions of training data by the model\n",
    "train_labels_predict=model.predict(train_values)\n",
    "#\n",
    "plt.scatter(train_labels,train_labels_predict)\n",
    "plt.ylabel('Predicted Taining Data',fontsize=20)\n",
    "plt.xlabel('True Training Data',fontsize=20)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.xlim(0,120)\n",
    "plt.ylim(0,120)\n",
    "plt.axline([0, 0], [1, 1],color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the predictions of only test data by the model\n",
    "test_labels_predict=model.predict(test_values)\n",
    "#np.shape(test_labels_predict)\n",
    "#np.shape(test_labels)\n",
    "# Plot the predictions of only test data by the model\n",
    "plt.scatter(test_labels,test_labels_predict)\n",
    "plt.ylabel('Predicted Test Data',fontsize=20)\n",
    "plt.xlabel('True Test Data',fontsize=20)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.xlim(0,120)\n",
    "plt.ylim(0,120)\n",
    "plt.axline([0, 0], [1, 1],color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINITION OF THE MODEL\n",
    "# The weights of our neural network will be initialized in a random manner, using a seed allows for reproducibility\n",
    "kernel_init = initializers.RandomNormal(seed=0)\n",
    "bias_init = initializers.Zeros()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(16, activation='relu', input_shape=(train_values.shape[1], ), kernel_initializer=kernel_init, \n",
    "                bias_initializer=bias_init))\n",
    "#model.add(Dense(32, activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init))\n",
    "model.add(Dense(16, activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init))\n",
    "\n",
    "#model.add(Dense(128, activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init))\n",
    "model.add(Dense(16, activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init))\n",
    "\n",
    "# model.add(Dense(128, activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init))\n",
    "model.add(Dense(1, kernel_initializer=kernel_init, bias_initializer=bias_init))\n",
    "\n",
    "# DEFINITION OF THE OPTIMIZER\n",
    "EPOCHS = 20000 # Number of EPOCHS\n",
    "my_learning_rate = 0.001\n",
    "my_decay_rate = my_learning_rate / EPOCHS\n",
    "\n",
    "# optimizer = optimizers.RMSprop(0.001) # RMSProp\n",
    "# optimizer = optimizers.Adam(0.001) # AdaM\n",
    "optimizer = optimizers.SGD(learning_rate=my_learning_rate,decay=my_decay_rate) # SGD\n",
    "\n",
    "# This line matches the optimizer to the model and states which metrics will evaluate the model's accuracy\n",
    "model.compile(loss='mae', optimizer=optimizer, metrics=['mae'])\n",
    "model.summary()\n",
    "\n",
    "#TRAINING\n",
    "# EPOCH REAL TIME COUNTER CLASS\n",
    "class PrintEpNum(keras.callbacks.Callback): # This is a function for the Epoch Counter\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write(\"Current Epoch: \" + str(epoch+1) + \" Training Loss: \" + \"%4f\" %logs.get('loss') \n",
    "                         + \" Validation Loss: \" + \"%4f\" %logs.get('val_loss')\n",
    "                         + '                                       \\r') # Updates current Epoch Number\n",
    "        \n",
    "# HISTORY Object which contains how the model learned\n",
    "\n",
    "# Training Values (Properties), Training Labels (Known Young's Moduli) \n",
    "history = model.fit(train_values, train_labels, batch_size=train_values.shape[0], \n",
    "                    epochs=EPOCHS, verbose = False, shuffle=False, validation_split=0.1, callbacks=[PrintEpNum()])\n",
    "\n",
    "\n",
    "# PLOTTING HISTORY USING MATPLOTLIB\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Abs Error')\n",
    "plt.plot(history.epoch, np.array(history.history['loss']),label='Loss on training set') \n",
    "plt.plot(history.epoch, np.array(history.history['val_loss']),label = 'Validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions by GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the predictions of training data by the model\n",
    "train_labels_predict=model.predict(train_values)\n",
    "#\n",
    "plt.scatter(train_labels,train_labels_predict)\n",
    "plt.ylabel('Predicted Taining Data',fontsize=20)\n",
    "plt.xlabel('True Training Data',fontsize=20)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.xlim(0,120)\n",
    "plt.ylim(0,120)\n",
    "plt.axline([0, 0], [1, 1],color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the predictions of only test data by the model\n",
    "test_labels_predict=model.predict(test_values)\n",
    "#np.shape(test_labels_predict)\n",
    "#np.shape(test_labels)\n",
    "# Plot the predictions of only test data by the model\n",
    "plt.scatter(test_labels,test_labels_predict)\n",
    "plt.ylabel('Predicted Test Data',fontsize=20)\n",
    "plt.xlabel('True Test Data',fontsize=20)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.xlim(0,120)\n",
    "plt.ylim(0,120)\n",
    "plt.axline([0, 0], [1, 1],color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loss and validation loss for the RMSprop algorithm, respectively, are 2.61 and 123.80. The model is overfitting to the training data because the validation loss is significantly greater than the training loss. This indicates that the model does not adapt well to brand-new, unforeseen data. The model design may be too complicated, which can lead to overfitting, as one potential explanation for this. Another factor that could lead to overfitting is a sample that is too tiny or noisy. In general, excellent adaptation performance should be indicated by a validation loss that is less than the training loss.\n",
    "\n",
    "The training loss and validation loss for the Adam algorithm, respectively, are 0.28 and 107.96. Although compared to the RMSprop optimizer, the validation loss is still greater than the training loss. This indicates that the model may be more successful than RMSprop at generalizing to novel data. Adam's ability to adjust the learning rate for each parameter based on the historical first and second moments of the gradients may be one explanation for this, as it can result in a quicker convergence and improved performance when compared to RMSprop. The validation loss' exact value, however, is still quite high, suggesting that the model may not be operating as well as it could.\n",
    "\n",
    "The training loss and validation loss for the Gradient Descent algorithm are 3.79 and 36.69, respectively. Compared to the other two optimizers, the difference between the validation loss and the training loss is comparatively modest, suggesting that the model may not be overfitting the training data as much. The validation loss' exact value, however, is still quite high, suggesting that the model may not be operating as well as it could. Furthermore, the Gradient Descent optimizer employs a comparatively low learning rate in comparison to the other two optimizers, which may result in delayed convergence and poorer accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# DEFINITION OF THE MODEL\n",
    "# The weights of our neural network will be initialized in a random manner, using a seed allows for reproducibility\n",
    "kernel_init = initializers.RandomNormal(seed=0)\n",
    "bias_init = initializers.Zeros()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(16, activation='relu', input_shape=(train_values.shape[1], ), kernel_initializer=kernel_init, \n",
    "                bias_initializer=bias_init))\n",
    "model.add(Dense(32, activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init))\n",
    "model.add(Dense(128, activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init))\n",
    "# model.add(Dense(128, activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init))\n",
    "model.add(Dense(1, kernel_initializer=kernel_init, bias_initializer=bias_init))\n",
    "\n",
    "# DEFINITION OF THE OPTIMIZER\n",
    "my_learning_rate = np.array([0.01,0.001,0.0001])\n",
    "\n",
    "for it in my_learning_rate:\n",
    "    EPOCHS = 20000 # Number of EPOCHS\n",
    "    my_decay_rate = it / EPOCHS\n",
    "    # optimizer = optimizers.RMSprop(0.001) # RMSProp\n",
    "    optimizer = optimizers.Adam(learning_rate=it,decay=my_decay_rate)#0.001) # AdaM\n",
    "    # optimizer = optimizers.SGD(0.001) # SGD\n",
    "    # This line matches the optimizer to the model and states which metrics will evaluate the model's accuracy\n",
    "    model.compile(loss='mae', optimizer=optimizer, metrics=['mae'])\n",
    "    model.summary()\n",
    "    #TRAINING\n",
    "    # EPOCH REAL TIME COUNTER CLASS\n",
    "    class PrintEpNum(keras.callbacks.Callback): # This is a function for the Epoch Counter\n",
    "        def on_epoch_end(self, epoch, logs):\n",
    "            sys.stdout.flush()\n",
    "            sys.stdout.write(\"Current Epoch: \" + str(epoch+1) + \" Training Loss: \" + \"%4f\" %logs.get('loss') \n",
    "                         + \" Validation Loss: \" + \"%4f\" %logs.get('val_loss')\n",
    "                         + '                                       \\r') # Updates current Epoch Number\n",
    "    EPOCHS = 20000 # Number of EPOCHS\n",
    "\n",
    "    # HISTORY Object which contains how the model learned\n",
    "    # Training Values (Properties), Training Labels (Known Young's Moduli) \n",
    "    history = model.fit(train_values, train_labels, batch_size=train_values.shape[0], \n",
    "                    epochs=EPOCHS, verbose = False, shuffle=False, validation_split=0.1, callbacks=[PrintEpNum()])\n",
    "    # PLOTTING HISTORY USING MATPLOTLIB\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Abs Error')\n",
    "    plt.plot(history.epoch, np.array(history.history['loss']),label='Loss on training set') \n",
    "    plt.plot(history.epoch, np.array(history.history['val_loss']),label = 'Validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Learning rate = 0.01:\n",
    "\n",
    "Training loss: 2.275096\n",
    "Validation loss: 66.994095\n",
    "\n",
    "Given the significant training loss, it is likely that the model does not accurately match the training set of data. The much greater validation loss, however, suggests that the model is also failing on data that has not yet been seen. Overfitting is a situation in which a model matches training data too closely because it is too complicated, which leads to subpar extension abilities.\n",
    "Justification: Because the model weights are changed too quickly at a high learning rate of 0.01 during training, the convergence is unreliable. This could lead to an overfitting of the model to the training set, which would lead to high training loss and high validation loss.\n",
    "\n",
    "For Learning rate = 0.001:\n",
    "\n",
    "Training loss: 0.349745\n",
    "Validation loss: 67.116203\n",
    "\n",
    "Although the validation loss is still very high despite the training loss being lower than in the prior situation, this indicates weak adaptation performance. This indicates that the model is still somewhat overfitting, albeit less so than before.\n",
    "Justification: A lower learning rate of 0.001 helps to stabilize the model training, leading to slower but more consistent updates to the weights. This may lessen the amount of overfitting, but overfitting may still happen if the learning rate is too great.\n",
    "\n",
    "For Learning rate = 0.0001:\n",
    "\n",
    "Training loss: 0.014580\n",
    "Validation loss: 66.498611\n",
    "\n",
    "The model appears to be performing better on both the training and validation data because both the training loss and the validation loss are considerably lower than in the prior scenarios. This shows that the model is less overfitting and is better able to apply to new data.\n",
    "Justification: By further slowing weight changes with a very low learning rate of 0.0001, the model can settle more gradually and avoid overfitting. However, if the learning rate is too low, the model may settle too slowly and require more training time, possibly resulting to an unsatisfactory result.\n",
    "\n",
    "As the learning rate declines, it can also be seen that the training loss reduces while the validation loss rises. This indicates that using a very low learning rate could lead to the model being overfit on the training data, which would have a negative impact on its success on fresh data. A high learning rate, on the other hand, might make the model agree rapidly but not necessarily to the best answer. In order to obtain the best results, it is crucial to meticulously adjust the learning rate and other hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# DEFINITION OF THE MODEL\n",
    "# The weights of our neural network will be initialized in a random manner, using a seed allows for reproducibility\n",
    "kernel_init = initializers.RandomNormal(seed=0)\n",
    "bias_init = initializers.Zeros()\n",
    "\n",
    "layers = np.array([16,32,64])\n",
    "for nul in layers:\n",
    "    model = Sequential()\n",
    "    model.add(Dense(nul, activation='relu', input_shape=(train_values.shape[1], ), kernel_initializer=kernel_init, \n",
    "                bias_initializer=bias_init))\n",
    "    model.add(Dense(32, activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init))\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init))\n",
    "    # model.add(Dense(128, activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init))\n",
    "    model.add(Dense(1, kernel_initializer=kernel_init, bias_initializer=bias_init))\n",
    "    \n",
    "    # DEFINITION OF THE OPTIMIZER\n",
    "    my_learning_rate = 0.01\n",
    "    EPOCHS = 20000 # Number of EPOCHS\n",
    "    my_decay_rate = my_learning_rate / EPOCHS\n",
    "    # optimizer = optimizers.RMSprop(0.001) # RMSProp\n",
    "    optimizer = optimizers.Adam(learning_rate=my_learning_rate,decay=my_decay_rate)#0.001) # AdaM\n",
    "    # optimizer = optimizers.SGD(0.001) # SGD\n",
    "    # This line matches the optimizer to the model and states which metrics will evaluate the model's accuracy\n",
    "    model.compile(loss='mae', optimizer=optimizer, metrics=['mae'])\n",
    "    model.summary()\n",
    "    #TRAINING\n",
    "    # EPOCH REAL TIME COUNTER CLASS\n",
    "    class PrintEpNum(keras.callbacks.Callback): # This is a function for the Epoch Counter\n",
    "        def on_epoch_end(self, epoch, logs):\n",
    "            sys.stdout.flush()\n",
    "            sys.stdout.write(\"Current Epoch: \" + str(epoch+1) + \" Training Loss: \" + \"%4f\" %logs.get('loss') \n",
    "                         + \" Validation Loss: \" + \"%4f\" %logs.get('val_loss')\n",
    "                         + '                                       \\r') # Updates current Epoch Number\n",
    "    EPOCHS = 20000 # Number of EPOCHS\n",
    "\n",
    "    # HISTORY Object which contains how the model learned\n",
    "    # Training Values (Properties), Training Labels (Known Young's Moduli) \n",
    "    history = model.fit(train_values, train_labels, batch_size=train_values.shape[0], \n",
    "                    epochs=EPOCHS, verbose = False, shuffle=False, validation_split=0.1, callbacks=[PrintEpNum()])\n",
    "    # PLOTTING HISTORY USING MATPLOTLIB\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Abs Error')\n",
    "    plt.plot(history.epoch, np.array(history.history['loss']),label='Loss on training set') \n",
    "    plt.plot(history.epoch, np.array(history.history['val_loss']),label = 'Validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:\n",
    "\n",
    "After a few thousand epochs, the training loss for the 16 units in the first layer stabilizes at about 2.3, but the validation loss stays elevated and varies around 67. This shows that the algorithm is overfitting to the training data and is unable to adapt well.\n",
    "\n",
    "While the validation loss stays high and varies around 71 for the 32 units in the first layer, the training loss drops off quickly in the early epochs and stabilizes at around 0.7 after a few thousand. Despite the reduced training loss, this suggests that the model is also overfitting to the training data.\n",
    "\n",
    "The training loss for the 64 units in the first layer drops quickly in the early epochs and stabilizes at about 1.4 after a few thousand epochs, while the validation loss reduces gradually and stabilizes at about 59 after about 20000 epochs. This suggests that the model can extend more effectively than the two preceding models.\n",
    "\n",
    "Justification:\n",
    "\n",
    "The model becomes more complicated as the number of units in the first layer rises, enabling it to recognize more intricate patterns in the data. However, if there are too many units, the model might overfit the training set and be less effective at generalizing to fresh data.\n",
    "\n",
    "\n",
    "With only 16 units, the model might not be able to uncover enough intricate patterns in the data to perform well in generalization. Despite the reduced training loss, the model may be overfit to the training data in the instance of 32 units due to its excess capacity. With 64 units, the model appears to be just complex enough to pick up on trends in the data and apply well to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# DEFINITION OF THE MODEL\n",
    "# The weights of our neural network will be initialized in a random manner, using a seed allows for reproducibility\n",
    "kernel_init = initializers.RandomNormal(seed=0)\n",
    "bias_init = initializers.Zeros()\n",
    "\n",
    "functions = ['relu','tanh','softmax']\n",
    "for no in functions:\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, activation= no, input_shape=(train_values.shape[1], ), kernel_initializer=kernel_init, \n",
    "                bias_initializer=bias_init))\n",
    "    model.add(Dense(32, activation= no , kernel_initializer=kernel_init, bias_initializer=bias_init))\n",
    "    model.add(Dense(128, activation= no , kernel_initializer=kernel_init, bias_initializer=bias_init))\n",
    "    # model.add(Dense(128, activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init))\n",
    "    model.add(Dense(1, kernel_initializer=kernel_init, bias_initializer=bias_init))\n",
    "    \n",
    "    # DEFINITION OF THE OPTIMIZER\n",
    "    my_learning_rate = 0.01\n",
    "    EPOCHS = 20000 # Number of EPOCHS\n",
    "    my_decay_rate = my_learning_rate / EPOCHS\n",
    "    # optimizer = optimizers.RMSprop(0.001) # RMSProp\n",
    "    optimizer = optimizers.Adam(learning_rate=my_learning_rate,decay=my_decay_rate)#0.001) # AdaM\n",
    "    # optimizer = optimizers.SGD(0.001) # SGD\n",
    "    # This line matches the optimizer to the model and states which metrics will evaluate the model's accuracy\n",
    "    model.compile(loss='mae', optimizer=optimizer, metrics=['mae'])\n",
    "    model.summary()\n",
    "    #TRAINING\n",
    "    # EPOCH REAL TIME COUNTER CLASS\n",
    "    class PrintEpNum(keras.callbacks.Callback): # This is a function for the Epoch Counter\n",
    "        def on_epoch_end(self, epoch, logs):\n",
    "            sys.stdout.flush()\n",
    "            sys.stdout.write(\"Current Epoch: \" + str(epoch+1) + \" Training Loss: \" + \"%4f\" %logs.get('loss') \n",
    "                         + \" Validation Loss: \" + \"%4f\" %logs.get('val_loss')\n",
    "                         + '                                       \\r') # Updates current Epoch Number\n",
    "    EPOCHS = 20000 # Number of EPOCHS\n",
    "\n",
    "    # HISTORY Object which contains how the model learned\n",
    "    # Training Values (Properties), Training Labels (Known Young's Moduli) \n",
    "    history = model.fit(train_values, train_labels, batch_size=train_values.shape[0], \n",
    "                    epochs=EPOCHS, verbose = False, shuffle=False, validation_split=0.1, callbacks=[PrintEpNum()])\n",
    "    # PLOTTING HISTORY USING MATPLOTLIB\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Abs Error')\n",
    "    plt.plot(history.epoch, np.array(history.history['loss']),label='Loss on training set') \n",
    "    plt.plot(history.epoch, np.array(history.history['val_loss']),label = 'Validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loss for the ReLU activation function begins at a comparatively high value of 2.27, but it rapidly drops over the course of the first few epochs and levels off at about 0.6. The validation loss declines more slowly, tapering off at about 20, after beginning at a much higher number of 66.99. Given that the validation loss is significantly greater than the training loss, it is likely that the model has overfitted the training set.\n",
    "\n",
    "The training loss and validation loss for the tanh activation function both quickly decline within the first few epochs, achieving relatively low values of 0.08 and 62.77, respectively. The validation loss varies between 60 and 70, indicating that the model is not overfitting the data, while the training loss continues to steadily decline and levels off at around 0.04.\n",
    "\n",
    "We see that the training loss for the softmax activation function begins at a high value and declines gradually but steadily, flattening off at about 36. The validation loss begins at a greater number and gradually declines to 80 with fewer fluctuations.\n",
    "\n",
    "The effectiveness of a neural network depends on the activation function that is selected. ReLU is a widely used activation function that has shown to be successful in many situations, but if it is not correctly regularized, it can result in overfitting. Tanh is a symmetric activation function with an outcome that is zero-centered and limited between -1 and 1, which can improve convergence. As the output values are not bounded, Softmax may not be appropriate for regression tasks, despite being frequently used in multi-class categorization issues.\n",
    "\n",
    "\n",
    "In general, we find that the tanh activation function outperformed ReLU and softmax in terms of both training and confirmation loss. This implies that tanh is the best activation function for this specific issue. It is crucial to remember that the selection of the activation function can vary depending on the particular issue and dataset, and that the best option is frequently determined through trial and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
